1) Define the packages for imputation to be used
  (see this website for reference --> 
   http://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/)

2) Move the rescaling/normalization/imputation step inside the SL LOOP

3) Allow for multiple options for imputation methods to be used

4) Fix the labeling bug in the histogram plots (now the plots are with densities and not absolute counts)

5) Making the SL objects "lighter", so we don't need to save workspaces of many GBs
   if M=10K and more than 20-30 algorithm variants are used
   
6) Start to generate a list of wrappers to make the SL.library more comprehensive, and select one or two classes of prediction algorithms (e.g., glm, glmnet) to be expanded with many options with the wrappers

7) Start to analyze (cleaning, harmonization,..) the new dataset Ivo sent 
   (https://umich.instructure.com/courses/38100/files/folder/Case_Studies/15_ALS_CaseStudy)

8) Test the imputation methods by eliminating 20-30% of the observations randomly from the dataset and compare the results vs the whole dataset
