---
title: "CBDA-SL and Knockoff Filter Results on the Null Dataset - n=300 p=900, 9000 jobs x 10 experiments - MSE and Accuracy as ranking metrics for CBDA"
author: "Simeone Marino, Ivo Dinov, Jiachen Xu"
date: "`r format(Sys.time(), '%b %d %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Some useful information

This is a summary of a set of 1 experiments using a LONI pipeline workflow file that performs 3000 independent jobs, each one with the CBDA-SL and the knockoff filter feature mining strategies.
Each experiments has a total of 9000 jobs and is uniquely identified by 6 input arguments: # of jobs [M], % of missing values [misValperc], min [Kcol_min] and max [Kcol_max] % for FSR-Feature Sampling Range, min [Nrow_min] and max [Nrow_max] % for SSR-Subject Sampling Range.

This document has the final results, by experiment. See <https://drive.google.com/file/d/0B5sz_T_1CNJQWmlsRTZEcjBEOEk/view?ths=true> for some general documentation of the CBDA-SL project and github <https://github.com/SOCR/CBDA> for some of the code.

```{r Loading the Null_5_new Dataset, include=FALSE, echo=FALSE}
# # Here I load the dataset [not executed]
# Null_dataset_3_new = read.csv("C:/Users/simeonem/Documents/CBDA-SL/ExperimentsNov2016/Null/Null_5/Null_dataset_3_5.txt",header = TRUE)

```

Features selected by both the knockoff filter and the CBDA-SL algorithms are shown as spikes in the histograms shown below. I list the top features selected, set to 15 here.

```{r Set the location of the arguments file and the workspace directory, include=FALSE}
#arg_file = as.array('C:/Users/simeonem/Documents/CBDA-SL/Cranium/arg_Null_new.txt')
arg_file = as.array('C:/Users/simeonem/Documents/CBDA-SL/Cranium/exp_specs.txt')
#workspace_directory<-setwd("C:/Users/simeonem/Documents/CBDA-SL/ExperimentsNov2016/KO/Dec5/")
#workspace_directory<-c("C:/Users/simeonem/Documents/CBDA-SL/ExperimentsNov2016/Null9000/Null_5/n300p100")
workspace_directory<-c("C:/Users/simeonem/Documents/CBDA-SL/ExperimentsNov2016/Binomial9000/Jiachen/finalRData")
eval(parse(text=paste0("knitr::opts_knit$set(root.dir = '",workspace_directory,"')")))
#print(getwd())
library("caret")
#opts_knit$set(root.dir = 'c:/Users/kwilliams/git/ProjectX')

```

```{r Read the input arguments and the dataset from the specified file, include=FALSE}
eval(parse(text=paste0("arguments <- read.table(arg_file, header = TRUE)")))
```

```{r Set the list of experiments to analze, echo=FALSE}
#list_exps=1:dim(arguments)[1]; # all the experiments
#list_exps=list_exps[-1*c(21)] # the experiments to exclude
#list_exps = c(2:3,5:6)
list_exps = c(2:8,10:12)
```

```{r Save basic info in a temp file to speed up the script while looping through the experiments, echo=FALSE}
#label=c("Null_dataset_3_new")
label=c("Null_dataset_3_final")
#label=c("Null_dataset_3_1_final")
nonzero=c(10,20,30,40,50,60,70,80,90,100)
#nonzero=c(1,30,60,100,130,160,200,230,260,300)
#nonzero=c(1,100,200,300,400,500,600,700,800,900)
#nonzero=c(1,100,200,400,600,800,1000,1200,1400,1500)
eval(parse(text=paste0("save(arguments,workspace_directory,list_exps,label,file= \"~/temp_Null_dataset_3_new.RData\")")))
#print(getwd())
```

```{r Loop through each experiment to load the workspace and generate histograms/tables, echo=FALSE}
for (i in list_exps) {
  print(paste("EXPERIMENT",i),quote = FALSE)
print.table(arguments[i,])
#print(i)
  # print(workspace_directory)
  print(arguments[i,])
  M <-arguments[i,1]
  misValperc <- arguments[i,2]
  Kcol_min <- arguments[i,3]
  Kcol_max <- arguments[i,4]
  Nrow_min <- arguments[i,5]
  Nrow_max <- arguments[i,6]
  range_n <- eval(parse(text=paste0("c(\"",Nrow_min,"_",Nrow_max,"\")")))
  range_k <- eval(parse(text=paste0("c(\"",Kcol_min,"_",Kcol_max,"\")")))
  
  eval(parse(text=paste0("load(\"",workspace_directory,"/CBDA_SL_M",M,"_miss",misValperc,"_n",range_n,"_k",range_k,"_Light_",label,".RData\")")))
  #nonzero=c(1,30,60,100,130,160,200,230,260,300)

#  print(dim(Ypred))
# ## MSE GENERATION
#   sum=0
#   for (j in 1:M) {
#     for(s in 1:length(Ypred)) {
#       ## This checks first if the TYPE of the prediction object SL_Pred is NOT a double (which means that
#       ## the prediction step worked in the previous module. If it is NOT a double, the MSE is calculated
#       ## as the mean of the sum of the square of the difference between the prediction and the data to predict.
#       # If the SL_Pred is a double, SL_Pred_j is assigned to its MSE (very high value).
#       eval(parse(text=paste0("ifelse(typeof(SL_Pred_",j,") != \"double\",
#                              sum <- sum+sum(Ypred[",s,"] - SL_Pred_",j,"$pred[",s,"])^2,sum <- SL_Pred_",j,")")))
#       ## This step makes the final calculation of the MSE and it labels it with a j --> MSE_j
#       eval(parse(text=paste0("MSE_",j," <- sum/length(Ypred)")))
#       ## This resets the sum to 0 for the next MSE calculation
#       sum = 0;
#       }
#     }

  #  GENERATING THE ARRAY OF MSE FOR ALL THE M SL OBJECTS
  # MSE=0;
  # for (j in 1:M) {
  #   eval(parse(text=paste0("MSE[j] <- MSE_",j)))
  # }

  # MSE RANKING
  #for (s in seq(10,M,M/10)){
  #s=M;
  s=9000;
  MSE_temp <- NULL
  MSE_sorted_temp <- NULL
  MSE_temp <- data.frame(mse=MSE[1:s],k_set=1:s)
  MSE_sorted_temp <- MSE_temp[order(-MSE_temp$mse),]

  ## DEFINE HERE THE TOP # OF COVARIATES TO LIST in the MODEL MINING STEP
  # "top" is defined at the beginning (line 8) and represents the top MSEs to consider for
  # feature mining (ks). Each one will have a set of best features with their relative highest frequencies
  top=1000
  eval(parse(text=paste0("k_top_",top,"_temp <- NULL")))
  for (r in 1:top){
    eval(parse(text=paste0("k_top_",top,"_temp <- c(k_top_",top,"_temp, k",MSE_sorted_temp$k_set[r],")")))
  }
  save(s,top,list_exps, file = "exp_specs.RData")
  #  GENERATING THE ARRAY OF ACCURACY FOR ALL THE M SL OBJECTS
  # ACCURACY RANKING
  #for (s in seq(10,M,M/10)){
  Accuracy_temp <- NULL
  Accuracy_sorted_temp <- NULL
  Accuracy_temp <- data.frame(Accuracy=Accuracy[1:s],k_set=1:s)
  Accuracy_sorted_temp <- Accuracy_temp[order(-Accuracy_temp$Accuracy),]

  eval(parse(text=paste0("k_top_",top,"_temp_Accuracy <- NULL")))
  for (r in 1:top){
    eval(parse(text=paste0("k_top_",top,"_temp_Accuracy <- c(k_top_",top,"_temp_Accuracy,k",Accuracy_sorted_temp$k_set[r],")")))
  }
  
  # Cumulative KNOCKOFF results
  KO_sub <- NULL
  for (j in 1:s) {
    eval(parse(text=paste0("KO_sub <- c(KO_sub,KO_selected_",j,")")))
  }

#  print(dim(Xpred))
nonzero=c(10,20,30,40,50,60,70,80,90,100)
#nonzero=c(1,30,60,100,130,160,200,230,260,300)
#nonzero=c(1,100,200,300,400,500,600,700,800,900)
#nonzero=c(1,100,200,400,600,800,1000,1200,1400,1500)
#print(nonzero)

 
# GENERATE HISTOGRAM OF THE CUMULATIVE KNOCKOFF RESULTS FOR SINGLE EXPERIMENT 
x = KO_sub;
h=hist(x, plot = FALSE ,breaks=seq(min(x)-0.5, max(x)+0.5, by=1))
h$density = h$counts/sum(h$counts)*100
title_temp <- c("KNOCKOFF FILTER RESULTS")
plot(h,freq=FALSE,ylab='Density (%)',xlab='Feature #',main = title_temp,ylim=c(0,max(h$density)))

# GENERATE HISTOGRAM OF THE TOP # OF COVARIATES FOR SINGLE EXPERIMENT - MSE
eval(parse(text=paste0("x = k_top_",top,"_temp")))
h=hist(x, plot = FALSE ,breaks=seq(min(x)-0.5, max(x)+0.5, by=1))
h$density = h$counts/sum(h$counts)*100
title_temp <- c("CBDA-SL RESULTS - MSE metric")
plot(h,freq=FALSE,ylab='Density (%)',xlab='Feature #',main = title_temp,ylim=c(0,max(h$density)))
#readline("Press <return to continue")

# GENERATE HISTOGRAM OF THE TOP # OF COVARIATES FOR SINGLE EXPERIMENT - ACCURACY
eval(parse(text=paste0("x = k_top_",top,"_temp_Accuracy")))
h=hist(x, plot = FALSE ,breaks=seq(min(x)-0.5, max(x)+0.5, by=1))
h$density = h$counts/sum(h$counts)*100
title_temp <- c("CBDA-SL RESULTS - Accuracy metric")
plot(h,freq=FALSE,ylab='Density (%)',xlab='Feature #',main = title_temp,ylim=c(0,max(h$density)))


# RETRIEVE AND SAVE THE LABELS OF THE TOP [BEST] FEATURES
 Null_dataset_3_new = read.csv("Null_dataset_3_final.txt",header = TRUE)
  BEST=15; # how many top rows to show
  cols_to_eliminate=c(1)
  Top_features <- NULL
  eval(parse(text=paste0("Top_features=sort(table(k_top_",top,"_temp_Accuracy), decreasing = TRUE)")))
  eval(parse(text=paste0("Top_features_MSE=sort(table(k_top_",top,"_temp), decreasing = TRUE)")))
  
  Top_features_labels <- names(Null_dataset_3_new[-cols_to_eliminate])[as.numeric(names(Top_features))]
  Top_features_labels_MSE <- names(Null_dataset_3_new[-cols_to_eliminate])[as.numeric(names(Top_features_MSE))]
  
  eval(parse(text=paste0("Top_",BEST,"_features_labels <- names(Null_dataset_3_new[-cols_to_eliminate])[as.numeric(names(Top_features)[1:",BEST,"])]")))
  eval(parse(text=paste0("Top_",BEST,"_features_labels_MSE <- names(Null_dataset_3_new[-cols_to_eliminate])[as.numeric(names(Top_features_MSE)[1:",BEST,"])]")))
  
qa <-as.data.frame(Top_features[1:BEST])
qa_MSE <-as.data.frame(Top_features_MSE[1:BEST])

names(qa) <- c("CBDA","Frequency")
names(qa_MSE) <- c("CBDA","Frequency")

qa$Density <- 100*(qa$Frequency/sum(Top_features))
qa_MSE$Density <- 100*(qa_MSE$Frequency/sum(Top_features_MSE))

qa_ALL <- cbind(qa,qa_MSE)

print("TABLE with CBDA-SL & KNOCKOFF FILTER RESULTS")
print(c("EXPERIMENT",i))

Top_Knockoff_features=sort(table(KO_sub), decreasing = TRUE)
Top_Knockoff_features_labels <- as.numeric(names(Top_Knockoff_features)[1:BEST])
qa_ALL$Knockoff <- Top_Knockoff_features_labels
qa_ALL$KO_Count <- Top_Knockoff_features[1:BEST]
qa_ALL$KO_Density <- 100*(Top_Knockoff_features[1:BEST]/sum(Top_Knockoff_features))

names(qa_ALL) <- c("Accuracy","Count","Density","MSE","Count","Density","Knockoff","Count","Density")
#names(qa) <- c("CBDA","Frequency","Density","Knockoff","Density")
print(qa_ALL[1:BEST,], right = FALSE, row.names = FALSE)

#Null_dataset_3_final_MSE
#print("Nonzero Features")
#print(nonzero)
eval(parse(text=paste0("Null_dataset_3_final_exp_",i,"<-qa_ALL")))
eval(parse(text=paste0("k_top_",top,"_temp_exp_",i,"<- k_top_",top,"_temp")))
eval(parse(text=paste0("k_top_",top,"_temp_Accuracy_exp_",i,"<- k_top_",top,"_temp_Accuracy")))
eval(parse(text=paste0("KO_sub_exp_",i,"<- KO_sub")))

eval(parse(text=paste0("save(Null_dataset_3_final_exp_",i,",arguments,workspace_directory,list_exps,label,qa_ALL,k_top_",top,"_temp_Accuracy,k_top_",top,"_temp,k_top_",top,"_temp_exp_",i,",k_top_",top,"_temp_Accuracy_exp_",i,",KO_sub_exp_",i,", file= \"Null_dataset_3_final_exp_",i,".RData\")")))
# print(q)
rm(list = ls())
eval(parse(text=paste0("load(\"~/temp_Null_dataset_3_new.RData\")")))
cat("\n\n\n\n\n\n")
}
```

```{r GENERATE THE COMBINED TABLE OF RESULTS ACROSS EXPERIMENTS, echo=FALSE, eval = TRUE}
load("exp_specs.RData")
# top = 1000
# list_exps = c(2:3,5:6)
Accuracy_ALL <- NULL
MSE_ALL <- NULL
KO_ALL <- NULL
for (i in list_exps)
  {
  eval(parse(text=paste0("load('Null_dataset_3_final_exp_",i,".RData')")))
  eval(parse(text=paste0("Accuracy_ALL <- c(Accuracy_ALL,k_top_",top,"_temp_Accuracy_exp_",i,")")))
  eval(parse(text=paste0("MSE_ALL <- c(MSE_ALL,k_top_",top,"_temp_exp_",i,")")))
  eval(parse(text=paste0("KO_ALL <- c(KO_ALL,KO_sub_exp_",i,")")))
}

# GENERATE HISTOGRAM OF THE CUMULATIVE KNOCKOFF RESULTS FOR SINGLE EXPERIMENT 
x = KO_ALL;
h=hist(x, plot = FALSE ,breaks=seq(min(x)-0.5, max(x)+0.5, by=1))
h$density = h$counts/sum(h$counts)*100
title_temp <- c("KNOCKOFF FILTER RESULTS")
plot(h,freq=FALSE,ylab='Density (%)',xlab='Feature #',main = title_temp,ylim=c(0,max(h$density)))

# GENERATE HISTOGRAM OF THE TOP # OF COVARIATES FOR SINGLE EXPERIMENT - MSE
x = Accuracy_ALL
h=hist(x, plot = FALSE ,breaks=seq(min(x)-0.5, max(x)+0.5, by=1))
h$density = h$counts/sum(h$counts)*100
title_temp <- c("CBDA-SL RESULTS - Accuracy metric")
plot(h,freq=FALSE,ylab='Density (%)',xlab='Feature #',main = title_temp,ylim=c(0,max(h$density)))
#readline("Press <return to continue")

# GENERATE HISTOGRAM OF THE TOP # OF COVARIATES FOR SINGLE EXPERIMENT - ACCURACY
#print(arguments[i,])
x = MSE_ALL
h=hist(x, plot = FALSE ,breaks=seq(min(x)-0.5, max(x)+0.5, by=1))
h$density = h$counts/sum(h$counts)*100
title_temp <- c("CBDA-SL RESULTS - MSE metric")
plot(h,freq=FALSE,ylab='Density (%)',xlab='Feature #',main = title_temp,ylim=c(0,max(h$density)))

# The features listed above are then used to run a final analysis applying both the CBDA-SL and the knockoff filter. The ONLY features used for analysis are the ones listed above. 
# A final summary of the accuracy of the overall procedure is determined by using the CDBA-SL object on the subset of subjects held off for prediction. The predictions are then used to generate the confusion matrix.
# We basically combine the CBDA-SL & Knockoff Filter algorithms to first select the top features during the first round. Then, the second stage uses the top features to run a final predictive modeling step that can ultimately be tested for accuracy, sensitivity,.....


```



